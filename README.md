# Statistics quotes

Collected by Kevin Wright

These are quotations related to statistics in some way, a large portion of which have been extracted from source documents by me.  Reasonable care has been taken, but errors may exist.

## Averages

I abhor averages. I like the individual case. A man may have six meals one day
and none the next, making an average of three meals per day, but that is not a
good way to live.

> **Louis Brandeis**, http://www.brandeis.edu/legacyfund/bio.html


The per capita gross national product of a nation...as a measure of the
comfort of individual lives is about as apt, say, as deciding how to dress in
the morning according to the mean annual temperature of the region in which
one lives. If one lives in the tropics this would work well. But if one lives
in Minnesota, where the temperature might be thirty degrees below zero one
morning and one hundred degrees above zero another morning, one would be in
danger of dying of exposure or of prostration most of the time. The problem
with aggregate statistics is that they obscure both the extremes and patterns
of distribution.

> **Paul Gruchow**, *Grass Roots*, p. 44. [kw]


## Bayesian

The best way to convey to the experimenter what the data tell him about theta is to show
him a picture of the posterior distribution.

> *G. E. P. Box & G. C. Tiao*, *Bayesian Inference in Statistical Analysis* (1973)


If one could get some rational basis for obtaining the prior, then
there would be no problem.  But people have seminars these days about
something where someone says, 'I am going to use such and such a prior'.
Where does he get the prior?  It is not data based.  It is a mathematical
convenience or something like that.  It is not even obtained by using Bayes'
theorem. Why one should believe the outcome of using this seems to be a very
moot point.

> **Oscar Kempthorne**, A conversation with Oscar Kempthorne, *Statistical Science*, 1995, V 10, p. 333.


In the design of experiments, one has to use some informal prior knowledge.
How does one construct blocks in a block design problem for instance?  It is
stupid to think that use is not made of a prior.  But knowing that this prior
is utterly casual, it seems ludicrous to go through a lot of integration,
etc., to obtain 'exact' posterior probabilities resulting from this prior.
So, I believe the situation with respect to Bayesian inference and with
respect to inference, in general, has not made progress.  Well, Bayesian
statistics has led to a great deal of theoretical research.  But I don't see
any real utilizations in applications, you know.  Now no one, as far as I
know, has examined the question of whether the inferences that are obtained
are, in fact, realized in the predictions that they are used to make.

> **Oscar Kempthorne**, A conversation with Oscar Kempthorne, *Statistical Science*, 1995, V 10, p. 334.


I sometimes think that the only real difference between Bayesian and
non-Bayesian hierarchical modelling is whether random effects are labeled
with Greek or Roman letters.

> **Peter Diggle**, Comment on 'Bayesian analysis of agricultural field
experiments', (1999), J. Royal Statistical Society B, 61, 691--746.


Anything you can do with Bayesian estimators that you can't do with
frequentist estimators, you shouldn't.

> **Phillip Stark**, 16 June 1997, SIAM Conference on Mathematical Methods in Geosciences


The practicing Bayesian is well advised to become friends with as many
numerical analysts as possible.

> **James Berger**, *Statistical Decision Theory and Bayesian Analysis*, p. 202. [kw]


It might take just half an hour to write down the equation, but forever to do
the computation. As a result, frequentists used to say to Bayesians, "You're
wrong--but even if you weren't wrong, you still can't do the computation."

> **Brian Junker and Greg Wilson**, Bayes offers a new way to make sense of numbers, *Science*, 19 Nov 1999.


You just say "Bayesian," and people think you are some kind of genius.

> **Gary Churchill**, Bayes offers a new way to make sense of numbers, *Science*, 19 Nov 1999.


Bayesian computations give you a straightforward answer you can understand and
use. It says there is an X% probability that your hypothesis is true-not that
there is some convoluted chance that if you assume the null hypothesis is
true, you'll get a similar or more extreme result if you repeated your
experiment thousands of times. How does one interpret THAT!

> **Steven Goodman**, Bayes offers a new way to make sense of numbers, *Science*, 19 Nov 1999.


Bayesian methods are complicated enough, that giving researchers user-friendly
software could be like handing a loaded gun to a toddler; if the data is crap,
you won't get anything out of it regardless of your political bent.

> **Brad Carlin**, Bayes offers a new way to make sense of numbers, *Science*, 19 Nov 1999.


If a study, even a statistically significant one, suggests that pigs can fly,
Bayes's theorm allows researchers to combine the study's results
mathematically with hundreds of years of knowledge about the travel habits of
swine.

> **David Leonhardt**, *New York Times*, April 28, 2001 


If the prior distribution, at which I am frankly guessing, has little or no
effect on the result, then why bother; and if it has a large effect, then
since I do not know what I am doing how would I dare act on the conclusions
drawn?

> **Richard W Hamming**, The Art of Probability for Scientists and Engineers, 1991, p. 298.


I believe that there are many classes of problems where Bayesian analyses are
reasonable, mainly classes with which I have little acquaintance.

> **John Tukey**, The life and professional contributions of John W. Tukey, The
Annals of Statistics, 2001, Vol 30, p. 45.  [Note: Given Tukey's breadth of
knowledge, this may have been a subtle jab at Bayesians.]


These days the statistician is often asked such questions as "Are you a
Bayesian?" "Are you a frequentist?" "Are you a data analyst?" "Are you a
designer of experiments?". I will argue that the appropriate answer to ALL of
these questions can be (and preferably should be) "yes", and that we can see
why this is so if we consider the scientific context for what statisticians
do.

> **G.E.P. Box**


## Bootstrap


If there was ever an idea in statistics which evokes the reaction, "Why
the hell didn't I think of that," it has to be the bootstrap.

> **James R. Thompson**, 1997 Interface Proceedings [kw]


This computationally intensive operation [bootstrapping] is not one calculated
to endear a user to a database administrator.

> **Leland Wilkinson**, *The Grammar of Graphics*, p. 49. [kw]


The old rule of trusting the Central Limit Theorem if the sample size is
larger than 30 is just that--old. Bootstrap and permutation testing let
us more easily do inferences for a wider variety of statistics.

> **Tim Hesterberg**


## Consulting


[I ask consulting clients] to explain the project to me as though he or she
was explaining it to his or her parents. (A former colleague with considerable
consulting experience substitutes "grandparents" for "parents.")

> **Michael W. Tosset**, *Statistical Science*, Feb 98, p. 23. [kw]


Because no one becomes statistically self-sufficient after one semester of
study, I try to prepare students to become intelligent consumers of the
assistance that they will inevitably seek. Service courses train future
clients, not future statisticians.

> **Michael W. Tosset**, *Statistical Science*, Feb 98, p. 24. [kw]


As I left consulting to go back to the university, these were the perceptions
I had about working with data to find answers to problems:

(a) Focus on finding a good solution--that's what consultants get paid for.

(b) Live with the data before you plunge into modelling.

(c) Search for a model that gives a good solution, either algorithmic or data.

(d) Predictive accuracy on test sets is the criterion for how good the model is.

(e) Computers are an indispensible partner.

> **Leo Breiman**, *Statistical Science*, Vol. 16, p. 201. [kw]


## Correlation


The invalid assumption that correlation implies cause is probably among the
two or three most serious and common errors of human reasoning.

> **Stephen Jay Gould**,  *The Mismeasure of Man*


When noise is correlated it becomes music.

> **Anindya Roy**, personal communication


## Data

Scrutiny [of data] should take in the *names* of variates. Analysis of variates y1 to y5 is not statistics; analysis of plant height in centimetres, root weight in grams, etc., may be.
> **D. A. Preece**, In discussion of C. Chatfield, 'The initial examination of data', J. R. S. S. A. 148:234 (1985)


Small data ... Fits in memory on a laptop: <10 GB

Medium data ... Fits in memory on a server: 10 GB-1 TB

Big data ... Can't fit in memory on one computer: >1 TB

> **Hadley Wickham**, 2015, *Big Data Pipelines*


A *massive* data set is one for which the size,
heterogeneity, and general complexity cause serious pain for the analyst(s).

> **Kettenring**, 2001, Massive data sets ... reflections on a workshop, *Computing Science and Statistics, Proceedings of the 33rd Symposium on the Interface*, Vol 33.


The Dirty Data Theorem states that "real world" data tends to come
from bizarre and unspecifiable distributions of highly correlated variables
and have unequal sample sizes, missing data points, non-independent
observations, and an inderterminate number of inaccurately recorded values.

> **Unknown** *Statistically Speaking*, p. 282


The Titanic survival data seem to become to categorical data analysis what
Fisher's Iris data are to discriminant analysis.

> **Andreas Buja**, *Statistical Computing & Graphics Newsletter*, V10, N1, p 32. [kw]


Consideration needs to be given to the most appropriate data to be
collected. Often the temptation is to collect too much data and not give
appropriate attention to the most important. Filing cabinets and computer
files world-wide are filled with data that have been collected because they
may be of interest to someone in future. Most is never of interest to anyone
and if it is, its existence is unknown to those seeking the information, who
will set out to collect the data again, probably in a trial better designed
for the purpose. In general, it is best to collect only the data required to
answer the questions posed, when setting up the trial, and plan another trial
for other data in the future, if necessary.

> **Portmann & Ketata**, *Statistical Methods for Plant Variety Evaluation*, p. 15. [kw]


## Data Analysis


What accounts for the success of the [Iowa State] Stat Lab? I believe that it
is because it was not driven by the mathematics, but by actual problems in
biology, genetics, demography, economics, psychology, and so on.  To be sure,
a real problems give rise to abstract problems in statistical inference which
have a fascination of their own.  However, for statistics to remain viable,
statistical problems should have their genesis in real, data-related
problems.

> **Oscar Kempthorne**, A conversation with Oscar Kempthorne, *Statistical Science*, 1995, V 10, p. 335.


Scholars feel the need to present tables of model parameters in academic
articles (perhaps just as evidence that they ran the analysis they claimed to
have run), but these tables are rarely interpreted other than for their sign
and statistical significance.  Most of the numbers in these tables are never
even discussed in the text.  From the perspective of the applied data analyst,
R packages without procedures to compute quantities of scientific interest are
woefully incomplete.  A better approach focuses on quantities of direct
scientific interest rather than uninterpretable model parameters. ... For each
quantity of interest, the user needs some summary that includes a point
estimate and a measure of uncertainty such as a standard error, confidence
interval, or a distribution.  The methods of calculating these differ greatly
across theories of inference and methods of analysis.  However, from the
user's perspective, the result is almost always the same: the point estimate
and uncertainty of some quantity of interest.

> **Kousuke Imai, Gary King, Oliva Lau**, Toward a Common Framework for
Statistical Analysis and Development, *Journal of Computational and Graphical
Statistics*, 2008, v 17.


Doing applied statistics is never easy, especially if you want to get it
right.

> **Xiao-Li Meng**, 2005 Joint Statistical Meetings [kw]



Data analysis is a tricky business -- a trickier business than even tricky
data analysts sometimes think.

> **Bert Gunter**, S-news mailing list 6.6.2002 [kw]


A first analysis of experimental resuls should, I believe, invariably be
conducted using flexible data analytical techniques--looking at graphs
and simple statistics--that so far as possible allow the data to 'speak
for themselves'. The unexpected phenomena that such a approach offten uncovers
can be of the greatest importance in shaping and sometimes redirecting the
course of an ongoing investigation.

> **George Box**, *Signal to Noise Ratios, Performance Criteria, and Transformations*, Technometrics, 30, 1--17 [kw]


Exploratory data analysis can never be the whole story, but nothing else can
serve as the foundation stone--as the first step. (p. 3)

Unless exploratory data analysis uncovers indications, usually quantitative
ones, there is likely to nothing for confirmatory data analysis to
consider. (p. 3)

One thing the data analyst has to learn is how to expose himself to what his
data are willing--or even anxious--to tell him. Finding clues
requires looking in the right places and with the right magnifying
glass. (p. 21)

In data analysis, a plot of y against x may help us when we know nothing about
the logical connection from x to y--even when we do not know whether or
not there is one--even when we know that such a connection is
impossible. (p. 131)

Whatever the data ,we can try to gain understanding by straightening or by
flattening. When we succeed in doing one or both, we almost always see more
clearly what is going on. (p. 148)

It is a rare thing that a specific body of data tells us as clearly as we
would wish how it itself should be analyzed. (p. 397)

> **John Tukey**, *Exploratory Data Analysis*


## Data Quality

We have found that some of the hardest errors to detect by traditional methods
are unsuspected gaps in the data collection (we usually discovered them
serendipitously in the course of graphical checking).

> **Peter Huber**, 1994, Huge data sets, *Compstat '94: Proceedings*

"Happy families are all alike; every unhappy family is unhappy in its own way." -- Leo Tolstoy

And every messy data is messy in its own way - it's easy to define the
characteristics of a clean dataset (rows are observations, columns are
variables, columns contain values of consistent types).  If you start to look
at real life data you'll see every way you can imagine data being messy (and
many that you can't)!

> **Hadley Wickham**, 17 Jan 2008, R-help mailing list


What all practising data analysts agree on is that the proportion of project
time spent on data cleaning is huge.  Estimates of 75-90 percent have been
suggested.

> *Graphics of Large Datasets*, p. 20.


## Definitions


Suppose that Sir R. A. Fisher -- a master of public relations -- had not taken
over from ordinary English such evocative words as "sufficient", "efficient",
and "consistent" and made them into precisely defines terms of statistical
theory.  He might, after all, have used utterly dull terms for those
properties of estimators, calling them characteristics A, B, and C... Would his
work have had the same smashing influence that it did?  I think not, or at
least not as rapidly.

> **William H. Kruskal**, Formulas, Numbers, Words: Statistics in Prose, *The
American Scholar*, 1978; reprinted in D. Fiske, ed., *New Directions for
Methodology in Social and Behavioral Sciences*, San Francisco:
Jossey-Bass, 1981.


In former times, when the hazards of sea voyages were much more serious than
they are today, when ships buffeted by storms threw a portion of their cargo
overboard, it was recognized that those whose goods were sacrificed had a
claim in equity to indemnification at the expense of those whose goods were
safely delivered. The value of the lost goods was paid for by agreement
between all of those whose merchandise had been in the same ship. This sea
damage to cargo in transit was known as 'havaria' and the word came naturally
to be applied to the compensation money which each individual was called upon
to pay. From this Latin word derives our modern word 'average'.

> **M. J. Moroney**, *Facts from Figures*, p. 34.


The concept of randomness arises partly from games of chance. The word
'chance' derives from the Latin *cadentia* signifying the fall of a
die. The word 'random' itself comes from the French *randir* meaning to
run fast or gallop.

> **G. Spencer Brown**, *Probability and Scientific Inference*, Chapter VII (p. 35).


Monte Carlo method [Origin: after Count Montgomery de Carlo, Italian gambler
and random number generator (1792-1838)]. A method of jazzing up the action in
certain statistical and number-analytic environments by setting up a book and
inviting bets on the outcome of a computation.

> **Stan Kelly-Bootle**, *The Devil's DP Dictionary*. [Statistically Speaking, p. 213]


Statistics derives from a German term, *Statistik*, first used as a
substantive by the Gottingen professor Gottfried Achenwall in 1749.

> **Theodore M. Porter**, *The Rise of Statistical Thinking 1820-1900.* [Stastically Speaking, p. 253].


## Degrees of Freedom

One point which must be borne in mind when considering the relative efficiency
of randomized blocks and Latin squares is the number of error degrees of
freedom.  The six degrees of freedom for error provided by the 4x4 Latin
square have long been recognized as inadequate, at last by Fisher.  Something
of the order of 12 error degrees of freedom would appear desirable...unless
the effects under investigation are large in comparison with their
experimental errors.

> **Frank Yates**, (1935) Complex Experiments, *Supplement to the Journal of the Royal Statistical Society*, Vol 2, No. 1.


## Distributions


That the ten digits do not occur with equal frequency must be evident to any
one making much use of logarithmic tables, and noticing how much faster the
first pages wear out than the last ones.

> **Simon Newcomb**, 1881. Note on the frequencies of the different digits in natural numbers. *Amer. J. Math* 4, 39-40.



For a hundred years or so, mathematical statisticians have been in love with
the fact that the probability distribution of the sum of a very large number
of very small random deviations always converges to a normal
distribution. This infatuation tended to focus interest away from the fact
that, for real data, the normal distribution is often rather poorly realized,
if it is realized at all.

> *Numerical Recipes in C*, p 520 [kw]


## Duh


There are known knowns. These are things we know that we know. There are known
unknowns. That is to say, there are things that we know we don't know. But
there are also unknown unknowns. There are things we don't know we don't know.

> **Donald Rumsfeld**


I would not say that the future is necessarily less predictable than the
past. I think the past was not predictable when it started.

> **Donald Rumsfeld**


Forty percent of the sick leaves are on a Monday or Friday. This must change.

> The pointy-haired boss in a Dilbert cartoon


The statistics on sanity are that one out of every four Americans is suffering
from some form of mental illness. Think of your three best friends. If they
are okay, then it's you.

> **Rita Mae Brown**



A recent USA Today poll reports that 3 out of 4 people make up 75% of the
population.

> **David Letterman**


8 out of 5 people are innumerate!


> **Joseph Lucke**, S-news mailing list 10.16.01 [kw]


It is proven that the celebration of birthdays is healthy. Statistics show
that those people who celebrate the most birthdays become the oldest.

> **S. den Hartog**, Ph D. Thesis Universtity of Groningen


Anybody can win unless there happens to be a second entry.

> **George Ada**



## EM algorithm


The idea of optimization transfer is very appealing to me, especially since I
have never succeeded in fully understanding the EM algorithm.

> **Andrew Gellman**, *Journal of Computational and Graphical Statistics*, vol 9, p 49. [kw]


## Experiments


The traditional methods design of experiments are taught and/or discussed in
textbooks are not the ways design of experiments are or should be used for
real-world applications.

> **George Milliken**, Applied Statistics in Agriculture Conference, 2009



In medical research, too often the first published study testing a new
treatment provides the strongest evidence that will ever be found for that
treatment.  As better controlled studies--less vulnerable to the enthusiasms
of researchers and their sponsors--are then conducted, the treatment's
reported efficacy declines.  Years after the initial study [...] sometimes the
only remaining issue is whether the treatment is in fact harmful.  (Page 144)

> **Edward Tufte**, *Beautiful Evidence*


Alpha designs are the Chia Pets of our company and everybody wants one,
whether or not they need it.

> **DW**, 2000, personal communication [kw]



When nearest neighbor effects exist, the randomized complete block analysis
[can be] so poor as to deserver to be called catastrophic.  It [can not] even
be considered a serious form of analysis. It is extremely important to make
this clear to the vast number of researchers who have near religious faith in
the randomized complete block design.

> **Walt Stroup & D Mulitze**, *Nearest Neighbor Adjusted Best Linear Unbiased Prediction*, The American Statistician, 45, 194--200. [kw]


To call in the statistician after the experiment is done may be no more than
asking him to perform a postmortem examination: he may be able to say what the
experiment died of.

> **R.A. Fisher**, *Sankya*, Indian Statistical Congress, Vol 4, p. 17.



The statistician who supposes that his main contribution to the planning of an
experiment will involve statistical theory, finds repeatedly that he makes his
most valuable contribution simply by persuading the investigator to explain
why he wishes to do the experiment, by persuading him to justify the
experimental treatments, and to explain why it is that the experiment, when
completed, will assist him in his research.

> **Gertrude Cox**, Lecture in Washington 11 January 1951 *Statistically Speaking*, p. 84


An important distinction needs to be made between experimental designs using
complete blocks and those using incomplete blocks as regards to three
functions:
1. reducing the error mean square
2. adjusting estimates closer to true values, and 
3. refining rankings.

Complete blocks include all treatments whereas incomplete blocks include a
subset of the treatments. Both can reduce the residual error, but only
incomplete blocks can also adjust estimates of treatment effects closer to the
true values and thereby refine rankings among the treatments. These adjusted
estimates are usually more accurate than the raw averages over replicates, but
not always (exactly as is the case for accuracy gain through more
replication).  Likewise, these adjusted rankings are more likely to identify
correctly the best treatments. By declaring a smaller error but doing nothing
to sharpen estimates or refine rankings, complete block designs are rather
impotent. It is ironic that scientists rarely understand this huge difference
between getting one benefit or three from blocking.

> **Hugh G Gauch**, Three Strategies for Gaining Accuracy, *American Scientist*. [kw]


## Graphics


Had we started with this [quantile] plot, noticed that it looks straight and
not looked further, we would have missed the important features of the data.
The general lesson is important. Theoretical quantile-quantile plots are not a
panacea and must be used in conjunction with other displays and analyses to
get a full picture of the behavior of the data.

> **John M. Chambers, William S. Cleveland, Beat Kleiner, Paul A. Tukey**, *Graphical Methods for Data Analysis*, p. 212.


Visualization for large data is an oxymoron--the art is to reduce size before
one visualizes.  The contradiction (and challenge) is that we may need to
visualize first in order to find out how to reduce size.

> **Peter Huber**, Massive datasets workshop: Four years after, *Journal of Computational and Graphical Statistics*, Vol 8, 635--652.


Pie charts have severe perceptual problems... If you want to display some
data, and perceiving the information is not so important, then a pie chart is
fine.

> *S-Plus 2000 Programmer's Guide*, p. 349 [kw]



Merely drawing a plot does not constitute visualization. Visualization is
about conveying important information to the reader accurately. It should
reveal information that is in the data and should not impose structure on the
data.

> **W. Huber, X. Li, and R. Gentleman**, *Bioinformatics and Computational Biology Solutions using R and Bioconductor*, page 162 [kw]



While the dendrogram has been widely used to represent distances between
objects, it cannot really be considered to be a visualization method.
Dendrograms do not necessarily expose structure that exists in the data. In
many cases they impose structure on the data, and when that is the case it is
dangerous to interpret the observed structure.

> **W. Huber, X. Li, and R. Gentleman**, *Bioinformatics and Computational Biology Solutions using R and Bioconductor*, page 170. [kw]



Chartjunk does not achieve the goals of its propagators. The overwhelming fact
of data graphics is that they stand or fall on their content, gracefully
displayed. Graphics do not become attractive and interesting through the
addition of ornamental hatching and false perspective to a few bars.

> **Edward Tufte**, *The Visual Display of Quantitative Information*, p. 121. [kw]


A table is nearly always better than a dumb pie chart; the only
worse design than a pie chart is several of them, for then the viewer is asked
to compare quantities located in spatial disarray both within and between
pies...Given their low data-density and failure to order numbers along a
visual dimension, pie charts should never be used.

> **Edward Tufte**, *The Visual Display of Quantitative Information*, p. 178. [kw]


The preliminary examination of most data is facilitated by the use of
diagrams. Diagrams prove nothing, but bring outstanding features readily to
the eye; they are therefore no substitutes for such critical tests as may be
applied to the data, but are valuable in suggesting such tests, and in
explaining the conclusions founded upon them.

> **Ronald A Fisher**, *Statisical Methods for Research Workers* (p. 27)


Our statistical puritanism may incline us not to use shadows, but we confess
that a little bit of shadow is fun.

> **Dan Carr**, describing a 3-D plot. *Statistical Computing & Graphics Newsletter*, V. 10, N. 1, p. 2


We are not saying that the primary purpose of a graph is to convey numbers
with as many decimal places as possible. We agree with Ehrenberg (1975) that
if this were the only goal, tables would be better. The power of a graph is
its ability to enable one to take in the quantitative information, organize
it, and see patterns and structure not readily revealed by other means of
studying the data.

> **William Cleveland & Robert McGill**, 1984, Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Models, *Journal of the American Statistical Association*, 79, 531--554



There was a controversy [in the 1920s]...about whether the divided bar chart
or the pie chart was superior for portraying the parts of a whole. The contest
appears to have ended in a draw. We conclude that neither graphical form
should be used because other methods are demonstrably better.

> **William Cleveland & Robert McGill**, 1984, Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Models, *Journal of the American Statistical Association*, 79, 531--554


Our conclusion about patch maps [shaded maps of the U.S. states]
agrees with Tukey's (1979), who left little doubt about his opinions by
stating, 'I am coming to be less and less satisfied with the set of maps that
some dignify by the name *statistical map* and that I would gladly revile with
the name *patch map*'.

> **William Cleveland & Robert McGill**, 1984, Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Models, *Journal of the American Statistical Association*, 79, 531--554



There is no more reason to expect one graphs to 'tell all' than to expect one
number to do the same.

> **John Tukey**, *Exploratory Data Analysis*


It's generally considered bad practice to use more than six colors in a single
display.

> **Ross Ihaka**, http://tolstoy.newcastle.edu.au/R/help/04/12/9594.html [kw]


It's not easy to select more than a few clearly distinct colours. Also,
"distinct" is context-dependent, because:

What will be the spatial relationships of the different colours in your
output? You can successfully have fairly similar colours adjacent to each
other, since the contrast is more obvious when they're adjacent. However, if
you want to use colours to track identity and difference across scttered
points or patches, then you need bigger separations between colours, since you
want to be able to see easily that patch "A" here is of the same kind as patch
"A" there and different from patch "B" somwehere else, when mingled with
patches of other kinds.

And size matters. Big patches of similar colour (as on a map) can look quite
distinct, while the same colours used to plot filled circular blobs on a graph
might be barely distinguishable, and totally undistinguishable if used to plot
coloured "."s or "+"s.

It depends too on what you will be using to render the colours. Monitor
screens vary in their aility to render different colours distinctly, and so do
colour printers.

It's all very psycho-visual and success usually requires experimentation!

> **Ted Harding**, http://tolstoy.newcastle.edu.au/R/help/04/12/9520.html [kw]



The mere multiplicity of the attempts to deal with more than three continuous
dimensions by encoding additional variables into glyphs, Chernoff faces,
stars, Kleiner-Hartigan trees, and so on indicates that each of them has met
only with rather limited sucess.

> **Peter Huber**, Statistical graphics: history and overview, *Proceedings of the Fourth Annual Conference and Exposition*, p. 674 [kw]



Spatial patterns may be due to many sources of variation. In the context of
seeking explanations, John Tukey said that, "the unadjusted plot should not be
made." In other words, our perceptual/cognitive abilities are poor in terms of
adjusting for known source of variations and envisioning the resulting map. A
better strategy is to control for known sources of variation and/or adjust the
estimates before making the map.

> **Dan Carr**, Survey Research Methods Section newsletter, July 2002 [kw]



## Lotteries


My philosophy on the lottery is that while you actually have to buy a ticket
in order to win the lottery, buying a ticket does not significantly increase
your odds of winning.

> **Howie Smith**, personal communication [kw]



## Models - Mixed


It is a nontrivial exercies to correctly program even the simplest split-plot
model using PROC MIXED.

> **Jeremy Aldworth & Wherly P Hoffman**, Split-Plot Model With Covariate: A Cautionary Tale,
*The American Statistician*, 56:284--289. [kw]


What should be the distribution of random effects in a mixed model?  I think
Gaussian is just fine, unless you are trying to write a journal paper.

> **Terry Therneau**, Speaking at useR 2007. [kw]


The split-plot isn't everything.  It's the only thing.

> **Oliver Schabenberger**, Speaking at JSM 2008. [kw]


Sometimes the most important fit statistic you can get is 'convergence not
met'--it can tell you something is wrong with your model.

I think it is very comforting to know that the model you are fitting COULD
have happened.  What if the model you are attempting to fit could NOT have
happened?  Welcome to Generalized Estimating Equations.

I prefer smoothing to a parametric AR(1) structure--it is faster and better.

> **Oliver Schabenberger**, 2006 Applied Statistics in Agriculture Conference.
[kw]



This reminds me of the duality in statistics between computation and model
fit: better-fitting models tend to be easier to compute, and computational
problems often signal modeling problems.

*Andrew Gelman*, http://www.stat.columbia.edu/~cook/movabletype/archives/2006/02/fooled_by_rando.html [kw]



In the [animal-breeding] context, BLUP is the estimator of
E(s_i|y), where s_i is a [random] sire effect and y is the vector of data.  In
1950, Henderson gave a paper on this topic at a meeting of the Institute of
Mathematical Statistics and had referred to the technique as
"estimation" of random effects, a phrase that, Chuck [Henderson]
told me, went down like a lead balloon.  Statisticians simply do not estimate
random variables.  And that, along with a paper by Cochran (1951) [Improvement
by means of selection], is presumably and appropriately what led to the name
"predictor" rather than "estimator".

> **S R Searle**, 1991, C R Henderson, the Statistician; And His Contributions to Variance Components Estimation.  *J Dairy Sci*, 74, 4035--4044.  [kw]



Contrary to popular belief, we statisticians are not PROC MIXED model
factories that can spit out code for customized models for
split-strip-split-split-split plot designs and understand why the variance for
treatment 22 is 0 all before you can say hocus pocus.

> **DW**, personal communication (2000) [kw]



In many applications, the data analyst has a dilema: Should an effect be
classified as [fixed] and a BLUE obtained, or as [random] and a BLUP obtained?
The traditional distrinction between fixed and random effects is not helpful;
it may, in fact, lead the data analyst to choose the less efficient route.

> **Walter Stroup and D K Mulitze**, 1991. Nearest Neighbor Adjusted Best Linear Unbiased Prediction. *The American Statistician*, 45, 194--200. [kw]



In some traditions there is interest in 'estimating' the unobserved instances
of the random variables themselves, but rather than call them estimates the
fashion is to give them a different name such as BLUPs, posterior modes, and
several others. I favour a different name from 'estimate' as well, but my
preference is to call them 'residuals' since they do have exactly the same
status as ordinary residuals from a simple fixed effects regression model,
which are possibly the simplest special case.  Summing the squares of these
residuals and dividing by (a) the number of them or (b) the degrees of freedom
to estimate the variance component is the choice that has become known as (a)
maximum likelihood or (B) REML, although it is often presented in much more
guarded and qualified terms.

> **Bill Venables**, *Exegeses on Linear Models*. [kw]



## Models


Competent scientists do not believe their own models or theories, but rather
treat them as convenient fictions. ...The issue to a scientist is not whether
a model is true, but rather whether there is another whose predictive power is
enough better to justify movement from today's fiction to a new one.

> **Steve Vardeman**, 1987. Comment. *Journal of the American Statistical Association* 82 : 130-131. [kw]



If you just rely on one model, you tend to amputate reality to make it fit
your model.

> **David Brooks**, http://www.johndcook.com/blog/2011/12/01/amputating-reality/



Statistical models are sometimes misunderstood in epidemiology. Statistical
models for data are never true. The question whether a model is true is
irrelevant. A more appropriate question is whether we obtain the correct
scientific conclusion if we pretend that the process under study behaves
according to a particular statistical model.

> **Scott Zeger**, Statistical reasoning in epidemiology, *American Journal of Epidemiology*, 1991



It is not always convenient to remember that the right model for a population
can fit a sample of data worse than a wrong model -- even a wrong model with
fewer parameters. We cannot rely on statistical diagnostics to save us,
especially with small samples. We must think about what our models mean,
regardless of fit, or we will promulgate nonsense.

> **Leland Wilkinson**, *The Grammar of Graphics*, p. 335. [kw]



Fitting models to data is a bit like designing shirts to fit people. If you
fit a shirt too closely to one particular person, it will fit other people
poorly. Likewise, a model that fits a particular data set too well might not
fit other data sets well.

> **Rahul Parsa**, speaking to the Iowa SAS User's Group [kw]



All models are wrong, some models are useful.

> **George Box**, "Robustness in the Strategy of Scientific Model Building," in Lanner and Wilkerson, eds., *Robustness in Statistics* pp. 201-236



You might say that there's no reason to bother with model checking since all
models are false anyway. I do believe that all models are false, but for me
the purpose of model checking is not to accept or reject a model, but to
reveal aspects of the data that are not captured by the fitted model. (See
chapter 6 of Bayesian Data Analysis for some examples.)

> *Andrew Gelman*, http://www.stat.columbia.edu/~cook/movabletype/archives/2007/04/some_thoughts_o_3.html



When evaluating a model, at least two broad standards are relevant. One is
whether the model is consistent with the data. The other is whether the model
is consistent with the 'real world.'

> **Kenneth Bollen**, Structural Equations with Latent Variable



Entities are not to be multiplied without necessity.

> **William of Ockham** (1285--1349). [This is the basis for Occam's Razor: Among all models consistent with the facts, choose the simplest]



The point of a model is to get useful information about the relation between
the response and predictor variables. Interpretability is a way of getting
information. But a model does not have to be simple to provide reliable
information about the relation between predictor and response variables;
neither does it have to be a data model. The goal is not interpretability, but
accurate information.

> **Leo Breiman**, *Statistical Science*, Vol 16, p. 210. [kw]



The goals in statististics are to use data to predict and to get information
about the underlying data mechanism. Nowhere is it written on a stone tablet
what kind of model should be used to solve problems involving data. To make my
position clear, I am not against data models per se. In some situations they
are the most appropriate way to solve the problem. But the emphasis needs to
be on the problem and on the data.

Unfortunately, our field has a vested interest in data models, come hell or
high water. For instance, see Dempster's (1998) paper on modelling. His
position on the 1990 Census adjustment controversy is particularly
interesting. He admits that he doesn't know much about the data or the
details, but argues that the problem can be solved by a strong dose of
modelling. That more modeling can make error-ridden data accurate seems highly
unlikely to me.

> **Leo Breiman**, *Statistical Science*, Vol 16, p. 214. [kw]



## Outliers


All this discussion of deleting the outliers is completely backwards.  In my
work, I usually throw away all the good data, and just analyze the outliers.

> **Unknown pharmaceutical statistician**, The American Statistician, Vol 61, No 3, page 193. [kw]



I have often thought that outliers contain more information than the model.

> **Arnold Goodman**, 2005 Joint Statistical Meetings [kw]



I also learned to focus on the little things in the data that don't make
sense. Often the guys I hire will disregard them, thinking there must be
something wrong with the benchmark since it does not make sense. Being more
experienced I know that the things that don't make sense are the most
important data collected. Time and again, getting to the bottom of a minor
performance anomaly that should not exist reveals a design flaw or failure in
my understanding, and curing it leads to an advance in our performance that
was well worth having.

> **Hans Reiser**, http://kerneltrap.org/node/5654 [kw]


## Popular Culture


    What do you do for fun?
      Statistics.
    Not your job. Fun.
      Statistics.
    Of what?
      Sports, mostly. But I may calculate how many women here tonight have brown curly hair.

> **Sid Hansen** and a blind date on the TV show *Providence*



In Lake Wobegon, all the women are strong, the men are good looking, and all
the children are above average.

> **Garrison Keillor**, *A Prairie Home Companion*



It's a random pattern. That's the pattern.

> **Ed Chigliak** on the TV series *Northern Exposure*


Statistics prove near & far that folks who drive like crazy...are.

> **Burma Shave** sign in the Advertising Museum in Portland [kw]



## Probability

It is a part of probability that many improbable things will happen.

> **Agathon** (445 - 400 BC), [Chance News 7.02]



The chances of getting eaten up by a lion on Main Street aren't one in a
million, but once would be enough.

> **Lockwood's Long Shot**



The very name calculus of probabilities is a paradox. Probability opposed to
certainty is what we do not know, and how can we calculate what we do not
know?

> **Henri Poincare**, *The Foundations of Science*, Science and Hypothesis (p. 155).



Statitistics make it clear the fact that one's chances of being hurt by a bear
are far, far fewer than being struck by an auto almost anywhere, or being
mugged on a city street, for that matter. We pursue our automotive, urban
lives undaunted, often indifferent amid the police and ambulance sirens, but
in the Alaskan wilderness we lie awake worrying about bears.

> **John Kauffmann**, *Alaska's Brooks Range* [kw]


Cougars can be dangerous, especially to unsupervised children, but the chances
of becoming a cougar victim are far less than becoming a victim of lightning,
honeybees, moose, deer, pit bulls, football, snow-shoveling, or crossing the
street in front of your house. For some reason, we fear the true risks of
being killed far less than the remote risk of becoming prey.

> **Dennis L Olsen**, *Cougars*, page 46. [kw]


## Randomness

Anyone who considers arithmetic methods of producing random digits is, of
course, in a state of sin.

> **John von Neumann**, (1951) Various techniques used in connection with random digits, Applied Mathematics Series, no 12, 36-38.



The generation of random numbers is too important to be left to chance.

> **Robert R. Coveyou**, Oak Ridge National Laboratory, 1969



Landon Noll...has been tinkering with random number generators for nearly a
decade -- an exercise in bringing order to chaos. "There's a lot of beauty in
chaos," Noll says. "The Grand Canyon wouldn't be so popular if it was just a
uniform trench. The trick is controlling and managing chaos and turning it
into something useful."

> **Tom McNichol**, *Wired*, August, 2003, page 088. [kw]



Randomness is NOT the absence of a pattern.

> **Bill Venables**, 1999 S-Plus User's Conference [kw]


## Replication


It is instructive to consider the success rate for an average of N replicates
being closer to the true value than is the first replicate (or equivalently,
any one of the replicates). The reader may like to ponder the following three
questions before reading the next paragraph. (1) How often, for instance, is
the average of 2 replicates more accurate than 1 replicate? (2) How about the
average of 5 replicates? (3) And how many replicates would be needed to
achieve 90% or 95% confidence that their average is more accurate than 1
replicate? Curiously and regrettably, few scientists know the answers to these
practical questions.

Two replicates are more accurate than one replicate 60.8% of the time.  That
is, this level of replication helps accuracy 60.8% of the time, but hinders
accuracy the other 39.2% of the time. For five replicates, the success rate
climbs to 73.2%, but a single replicate is more accurate the other 26.8% of
the time. To increase replication's success rate to 90%, most of the
scientists that I queried guessed that 3 to 8 replicates would suffice, and my
own guess was 7. But the actual number is considerably larger, 40. To achieve
95% success from replicating, a daunting 162 replicates are needed, again far
beyond what most scientists would expect.

> **Hugh G Gauch**, Three Strategies for Gaining Accuracy, *American Scientist*. [kw]



At this meeting for the hybrid corn industry, we have no reservation
about recommending a design which consists of a single replicate of treatments
at a given location.  For some audiences, such a statement can severely damage
the reputation of the person making the statement.  University experiment
station personnel in particular regard replications within an environment as a
necessary part of good research.  They are not.  Sprague (1955) and many
others have shown most researchers otherwise. [Page 60]

The message from a statistician's point of view is very clear.
Replicate over environments, do not replicate within environments.  This is
not news.  At North Carolina State in the early 60s, any graduate student
interested in quantitative aspects of plant breeding and genetics had a
standard answer for the number of replicates needed in an experiment: use one
replicate if you're estimating means, and use two replicates if you're
estimating variances.  Implicit in the answer was, "the experiment will
be evaluated in more than one environment". [Page 62]

> **R. E. Stucker & D. R. Hicks**. Experimental Design and Plot Size Considerations for On-Farm Research, *Proceedings of the 46th Annual Corn  and Sorghum Industry Research Conference*, 1991.



## Regression


...the orthogonal least squares technique does *not* build in a hidden agenda
about "predicting" y from x or x from y. If I choose to think that x causes y,
or y causes x, that's my business -- and I do, in fact, choose to think this
way, at times. But the cause is not in the data, so all I want from the line
is a summary description of the data. The rest is up to me. That is why I use
the orthogonal least squares line. Compared to the regression line, the line
of means, it uses fewer hidden assumptions and is closer to a purely
descriptive statistic: *closer* to the facts and nothing but the facts.

> **Joel Levine**, *Exceptions Are The Rule*, p. 31 [kw]


Regression is a method which --

1. usually works, although it is imperfect and may sometimes go wrong -- like anything else;
2. sometimes works in the hands of skillful practitioners, but isn't suitable for routine use;
3. might work, but hasn't yet;
4. can't work.

Textbooks, courtroom testimony, and newspaper interviews often seem
to put regression into category (1). Although I have not had much luck in
finding good examples, my own confidence interval is bracketed by (2) and
(3). Option (4) seems too pessimistic.

> **David Freeman**, *Statistical Models and Shoe Leather*, Tech Report 217, Statistics Department, UC Berkely. [kw]


## Robust statistics

Just which robust/resistant methods you use is not important--what is
important is that you use some. It is perfectly proper to use both classical
and robust/resistant methods routinely, and only worry when they differ enough
to matter. But, when they differ, you should think hard.

> **John Tukey**, 1979 (From Doug Martin)


## Sampling


Mountain goats always travel single file. At least the one I saw did.

> **Unknown**



[John Kennedy] read every fiftieth letter of the thirty thousand coming weekly
to the White House, as well as a statistical summary of the entire batch, but
he knew that these were often as organized and unrepresentative as the pickets
on Pennsylvania Avenue.

> **Theodore Sorensen**, *Kennedy*. Found in *Sampling: Design and Analysis* by Sharon Lohr, page 23. [kw]



I have always thought that statistical design and sampling from populations
should be the first courses taught, but all elementary courses I know of start
with statisical methods or probability. To me, this is putting the cart before
the horse!

> **Walter Federer**, A Conversation with Walter T Federer, *Statistical Science*, 2005, Vol 20, p. 312. [kw]


## Significance, Hypothesis Testing, Power, p-values


The difference between "statistically significant" and "not
statistically significant" is not in itself necessarily statistically
significant. By this, I mean more than the obvious point about arbitrary
divisions, that there is essentially no difference between something
significant at the 0.049 level or the 0.051 level.

I have a bigger point to make. It is common in applied research--in the last
couple of weeks, I have seen this mistake made in a talk by a leading
political scientist and a paper by a psychologist--to compare two effects,
from two different analyses, one of which is statistically significant and one
which is not, and then to try to interpret/explain the difference. Without any
recognition that the difference itself was not statistically
significant.

Let me explain. Consider two experiments, one giving an estimated effect of 25
(with a standard error of 10) and the other with an estimate of 10 (with a
standard error of 10). The first is highly statistically significant (with a
p-value of 1.2%) and the second is clearly not statistically significant (with
an estimate that is no bigger than its s.e.).

What about the difference? The difference is 15 (with a s.e. of
sqrt(10^2+10^2)=14.1), which is clearly not statistically significant! (The
z-score is only 1.1.)

This is a surprisingly common mistake. The two effects seem sooooo different,
that it is hard for people to even think that their difference might be
explained purely by chance.

> **Andrew Gelman**, http://www.stat.columbia.edu/~cook/movabletype/archives/2005/06/the_difference.html"  
Also: The Difference Between "Significant" and "Not Significant" is not Itself Statistically Significant, *The American Statistician*, 2006, Vol 60, 328--331.


We must watch our own language.  For example, "Type I error" and "Type II
error" are meaningless and misleading terms.  Instead, try "chance of a false
alarm" and "a missed opportunity".

> **Deborah J. Rumsey**, Assessing Student Retention of Essential Statistical Ideas: Perspectives, Priorities, and Possibilities", *American Statistician*, Vol 62, No 1, p. 58. [kw]



A quotation of a p-value is part of the ritual of science, a sprinkling of the
holy waters in an effort to sanctify the data analysis and turn consumers of
the results into true believers.

> **William Cleveland**, *Visualizing Data*, p. 177. [kw]



The p-value is a concept so misaligned with intuition that no civilian can
hold it firmly in mind.  Nor can many statisticians.

> **Matt Briggs**, Why do statisticians answer silly questions that no one ever asks?", *Significance*, Vol 9, No 1, p. 30. [kw]



We should push for de-emphasizing some topics, such as statistical
significance tests--an unfortunate carry-over from the traditional elementary
statistics course. We would suggest a greater focus on confidence
intervals---these achieve the aim of formal hypothesis testing, often provide
additional useful information, and are not as easily misinterpreted.

> **Gerry Hahn et. al**, The Impact of Six Sigma Improvement--A Glimpse Into the Future of Statistics", *American Statistician*, August 1999. [kw]



We statisticians must accept much of the blame for cavalier attitudes toward
Type I errors. When we teach practitioners in other scientific fields that
multiplicity is not important, they believe us, and feel free to thrash their
data set mercilessly, until it finally screams "uncle" and relinquishes
significance. The recent conversion of the term "data mining" to mean a
statistical *good* rather than a statistical *evil* also contributes to the
problem.

> **Peter Westfall**. Applied Statistics in Agriculture (Proceedings of the 13th annual conference), page 5. [kw]


While the main emphasis in the development of power analysis has been to
provide methods for assessing and increasing power, it should also be noted
that it is possible to have too much power. If your sample is too large,
nearly any difference, no matter how small or meaningless from a practical
standpoint, will be "statistically significant". This can be
particularly problematic in applied settings, where courses of action are
determined by statistical results

...as I pointed out earlier, significance (in the statistical sense) is really
as much a function of sample size and experimental design as it is a function
of strength of relationship. With low power, you may be overlooking a really
useful relationship; with excessive power, you may be finding microscopic
effects with no real practical value.

Remember that a p-value merely indicates the probability of a particular set
of data being generated by the null model--it has little to say about the size
of a deviation from that model (especially in the tails of the distribution,
where large changes in effect size cause only small changes in p-values).

> **Clay Helberg**, http://www.execpc.com/~helberg/pitfalls/


Given what I know about data, models, and assumptions, I find more than 2
significant digits of printout for a p-value to be indefensible. (I actually
think 1 digit is about the max).

> **Terry Therneau**, S-news mailing list, 8 Nov 2000 [kw]



In the calculus of real statistical inference, and by that I mean actual data
problems (which S was designed for), all p-values < 10^-6 or so are
identical. This is one of the few areas in fact where I like SAS better: the
creators of their PROCs are smart enough to print these numbers as zero and
leave it at that. There are no Gaussian distributions in the real world, and
the central limit theorem has failed long, long before 10^-17.

> **Terry Therneau**, S-news mailing list, 4 Apr 2002 [kw]



It's a commonplace among statisticians that a chi-squared test (and, really,
any p-value) can be viewed as a crude measure of sample size: When sample size
is small, it's very difficult to get a rejection (that is, a p-value below
0.05), whereas when sample size is huge, just about anything will bag you a
rejection. With large n, a smaller signal can be found amid the noise.

In general: small n, unlikely to get small p-values. Large n, likely to find
something. Huge n, almost certain to find lots of small p-values.

> **Andrew Gelman**, http://www.stat.columbia.edu/~cook/movabletype/archives/2009/06/the_sample_size.html



Statisticians classically asked the wrong question--and were willing to
answer with a lie, one that was often a downright lie. They asked "Are
the effects of A and B different?" and they were willing to answer
"no".

All we know about the world teaches us that the effects of
A and B are always different--in some decimal place--for every A
and B. Thus asking "Are the effects different?" is foolish. What we
should be answering first is "Can we tell the direction in which the
effects of A differ from the effects of B?" In other words, can we be
confident about the direction from A to B? Is it "up",
"down" or "uncertain"?

> **John Tukey. The Philosophy of Multiple Comparisons**, *Statistical Science*, 6:100-116. [kw]

Work by Bickel, Ritov, and Stoker (2001) shows that goodness-of-fit tests have
very little power unless the direction of the alternative is precisely
specified. The implication is that omnibus goodness-of-fit tests, which test
in many directions simultaneously, have little power, and will not reject
until the lack of fit is extreme.

Residual analysis is similarly unreliable. In a discussion after a
presentation of residual analysis in a seminar at Berkeley in 1993, William
Cleveland, one of the fathers of residual analysis, admitted that it could not
uncover lack of fit in more than four to five dimensions. The papers I have
read on using residual analysis to check lack of fit are confined to data sets
with two or three variables. With higher dimensions, the interactions between
the variables can produce passable residual plots for a variety of models. A
residual plot is a goodness-of-fit test, and lacks power in more than a few
dimensions. An acceptable residual plot does not imply that the model is a
good fit to the data.

> **Leo Breiman**, *Statistical Science*, Vol 16, p. 203. [kw]


## Software:R

It would help if the standard statistical programs did not generate t
statistics in such profusion.  The programs might be written to ask, "Do you
really have a probability sample?", "By what standard would you judge a fitted
coefficient large or small?"  Or perhaps they could merely say, printed in
bold capitals beside each equation, "So What Else Is New?"

> **Donald M. McCloskey**, The Loss Function Has Been Mislaid: The Rhetoric of Significance Tests, *American Economic Review*, Vol 75, #2.


The documentation level of R is already much higher than average for open
source software and even than some commercial packages (esp. SPSS is notorious
for its attitude of "You want to do one of these things. If you don't
understand what the output means, click help and we'll pop up five lines of
mumbo-jumbo that you're not going to understand either.")

> **Peter Dalgaard**, R-help mailing list 4.2.2002 [kw]


## Software:S-Plus


Why can't the lme function speak English? It said to me:

Error in .C("matrixLog_pd",: subroutine matrixLog_pd: 2628 missing value(s) in argument 3. 

It should have said, "Hey, dummy, you forgot to put a 'random' argument in the lme function."

> **Kevin Wright**



S has forever altered the way people analyze, visualize, and manipulate data
.... S is an elegant, widely accepted, and enduring software system, with
conceptual integrity, thanks to the insight, taste, and effort of John
Chambers.

> Association for Computing Machinery Software System Award



Tradition among experienced S programmers has always been that loops
(typically 'for' loops) are intrinsically inefficient: expressing computations
without loops has provided a measure of entry into the inner circle of S
programming.

> **John Chambers**, *Programming With Data*, p. 173. [kw]



While the distribution and publication of Version 2 [of S] was still evolving,
parallel research work was starting to shape the next major version. At first
this seemed to be a move away from S altogether: something called the
"Quantitative Programming Environment" was initially a separate
research project, aimed more explicitly at programming and trying to emphasize
that users need not be statistically sophisticated. By 1986, however, the
decision was made to merge this work with the facilities (especially the
graphics) underlying S, to produce a new version of S. (This explains, by the
way, why the main program for S is called Sqpe, another one of those little
puzzles for users.)

> **Unknown**, http://cm.bell-labs.com/cm/ms/departments/sia/S/history.html



I was profoundly disappointed when I saw that S-PLUS 4.5 now provides Type III
sums of squares as a routine option for the summary method for aov objects. I
note that it is not yet available for multistratum models, although this has
all the hallmarks of an oversight (that is, a bug) rather than common sense
seeing the light of day. When the decision was being taken of whether to
include this feature, because the FDA requires it a few of my colleagues and I
were consulted and our reply was unhesitatingly a clear and unequivocal No,
but it seems the FDA and SAS speak louder and we were clearly outvoted. ...

> **Bill Venables**, *Exegeses on Linear Models* (white paper). [kw]



Some of us feel that type III sum of squares and so-called LS-means are
statistical nonsense which should have been left in SAS.

> **Brian Ripley**, disucussing features of S-Plus, S-news 5.29.1999 [kw]


## Software:SAS


Overall, SAS is about 11 years behind R and S-Plus in statistical capabilities
(last year it was about 10 years behind) in my estimation.

> **Frank Harrell**, (SAS User, 1969-1991), R-help (September 2003)


I think it would be interesting to ask people who use the results from LSMEANS
to explain what the results represent. My guess is that less than 1% of the
people who use LSMEANS know what they in fact are.

> **Doug Bates**, R-help (Oct 16, 2003) [kw]


PROC ANOVA? Give me a break! They might as well be using an abacus.

> **DW**, personal communication [kw]



...it must still be said that S-PLUS offers the best environment and suite of
tools for actually doing linear modelling on the sorts of consulting jobs that
arise in practice. The area covered by just the four fitting functions lm,
aov, glm and nls is handled in SAS by an unbelievable array of PROCs each with
some special features and other special limitations. Even the notion of
"General linear model" in SAS simply means a linear model that is allowed to
have *both* factors *and* quantitative explanatory variables. Just how general
can you get?

> **Bill Venables**, *Exegeses on Linear Models* (white paper). [kw]


## Statisticians


The only useful function of a statistician is to make predictions, and thus to
provide a basis for action.

> **William Edwards Deming**, *Journal of the American Statistical Association* Quoted in W.A. Wallis' The Statical Research Group, 1942-1945 Volume 75, Number 370, June 1980 (p. 321)



Today . . .we have high-speed computers and prepackaged statistical routines
to perform the necessary calculations. . . . statistical software will no more
make one a statistician than would a scalpel turn one into a
neurosurgeon. Allowing these tools to do our thinking for us is a sure recipe
for disaster.

> **Good & Hardin**, *Common Errors in Statistics and How to Avoid Them*, p. ix


## Statistics profession


[I was 17 when] there was an item in *Barron's* saying if we would send
along a description of how we used their statistical material they would
publish some of them and pay $5. I wrote up something about how I used odd-lot
figures. That $5 was the only money I ever made using statistics.

> **Warren Buffett**, in *Warren Buffett Speaks* by Janet Lowe, page 129. [kw]



Statistics is, or should be, about scientific investigation and how to do it
better, but many statisticians believe it is a branch of mathematics.

> **George Box**, Found in *AmStat News*, Oct 2000, page 11.


There are aspects of statistics other than it being intellectually difficult
that are barriers to learning. For one thing, statistics does not benefit from
a glamorous image that motivates students to persist through tedious and
frustrating lessons...there are no TV dramas with a good-looking statsitician
playing the lead, and few mothers' chests swell with pride as they introduce
their son or daughter as "the statistician."

> **Chap T. Le and James R. Boen**, in *Health and Numbers: Basic Statistical Methods*


## John Tukey

One is so much less than two.

> **John Tukey**, Eulogizing his wife. Quoted in "The life and professional contributions of John W. Tukey", *The Annals of Statistics*, 2001, Vol 30, p. 46.



No one has ever shown that he or she had a free lunch. Here, of course,
"free lunch" means "usefulness of a model that is locally easy
to make inferences from".

If asymptotics are of any real value, it must be because they teach us
something useful in finite samples. I wish I knew how to be sure when this
happens.

> **John Tukey**, Issues relevant to an honest account of data-based inference, partially in the light of Laurie Davies' paper. [kw]



Box: We don't need robust methods. A good statistician (particularly a
Bayesian one) will model the data well and find the outliers.

Tukey: They ran over 2000 statistical analyses at Rothamsted last week
and nobody noticed anything. A red light warning would be most helpful.

> **George Box vs. John Tukey**, argument circa 1989. Cited by R. Douglas Martin, 1999 S-Plus Conference Proceedings [kw]


The best thing about being a statistician, is that you get to play in
everyone's backyard.

> **John Tukey**


Far better an approximate answer to the right question, which is often vague,
than the exact answer to the wrong question, which can always be made precise.

> **John Tukey**, *Ann. Math. Stat*. 33 (1962)


An approximate answer to the right question is worth far more than a precise
answer to the wrong one.

> **John Tukey**



Statistics is a science in my opinion, and it is no more a branch of
mathematics than are physics, chemistry, and economics; for if its methods
fail the test of experience--not the test of logic--they will be
discarded. (p. 15)

> **John Tukey**, in "The life and professional contributions of John W. Tukey" by David Brillinger, *The Annals of Statistics*, 2001, Vol 30


One Christmas Tukey gave his students books of crossword puzzles as
presents. Upon examining the books the students found that Tukey had removed
the puzzle answers and had replaced them with words of the sense: "Doing
statistics is like doing crosswords except that one cannot know for sure
whether one has found the solution." (p. 22)

> **John Tukey**, in "The life and professional contributions of John W. Tukey" by David Brillinger, *The Annals of Statistics*, 2001, Vol 30



A competent data analysis of an even moderately complex set of data is a thing
of trials and retreats, of dead ends and branches.

> **John Tukey**, Computer Science and Statistics: Proceedings of the 14th Symposium on the Interface, p. 4) [kw]



A sort of question that is inevitable is: "Someone taught my students
exploratory, and now (boo hoo) they want me to tell them how to assess
significance or confidence for all these unusual functions of the data. (Oh,
what can we do?)" To this there is an easy answer: TEACH them the JACKKNIFE.

> **John Tukey**, *The American Statistician*, Vol 34, No 1, p. 25.



Many students are curious about the '1.5 x IQR Rule';, i.e. why do we use Q1 -
1.5 x IQR (or Q3 + 1.5 x IQR) as the value for deciding if a data value is
classified as an outlier? Paul Velleman, a statistician at Cornell University,
was a student of John Tukey, who invented the boxplot and the 1.5 x IQR
Rule. When he asked Tukey, 'Why 1.5?', Tukey answered, 'Because 1 is too small
and 2 is too large.'

[Assuming a Gaussian distribution, about 1 value in 100 would be an
outlier. Using 2 x IQR would lead to 1 value in 1000 being an outlier.]

> **Unknown**, How to draw a box plot, http://exploringdata.cqu.edu.au/box_norm.htm



John's eye for detail was amazing. When we were preparing some of the material
for our book (which was published last year), it was most disconcerting to
have him glance at the data and question one value out of several thousand
points. Of course, he was correct and I had missed identifying this anomaly.

> **Kaye Basford**, http://stat.bell-labs.com/who/tukey/tributes.html


## Other


The original emblem of the Statistical Science Association of Canada
featured a waterfall and Canada geese. The designer offered the
following comments in *Statistical Science*, Feb 1999, p. 87:

[Canada geese] fly across continents from the Arctic to the tropic with a
message from the north and bringing happiness to every one...So also the
Statistical Science Association of Canada and the Canadian Journal of
Statistics shall spread a message across continents and shall bring home
happiness....Like the gushing water at the Horseshoe Falls, Statistical
Science Association, through its journal, shall gush out the vast reservoir of
knowledge, radiating a beautiful rainbow across the horizon of scientific
activities.

> **Unknown**


Ronald Fisher gave us LSD 40 years before anyone ever heard of Timothy Leary

> **Kevin Wright**



Genotype by environment interaction is really a fancy way for a corn breeder
to say, "I don't know what's going on."

> **Todd Piper**, personal communication, 2.15.00 [kw]



There are two books devoted solely to principal components, Jackson (1991) and
Jolliffe (1986), which we think overstates its value as a technique.

> **Venables & Ripley**, *Modern Applied Statistics with S*, 4th ed., page 305. [kw]



Statistics state the status of the state.

> **Leland Wilkinson**, *The Grammar of Graphics*, p. 165. [kw]


Econometrics has successfully predicted 14 of the last 3 economic depressions.

> **David Hand**, speaking at Interface 2000. [kw]



We feel that nothing can replace the value to a [corn] breeder of careful
study and understanding of his plants...More and more, we feel that grave
danger exists of statistics being used as a substitute for critical
observation and thought...Statistics have their place, a very important one,
but they can never serve as a substitute for close association with
plants. Their real value, it seems to use, is in measureing precisely what we
already know in a general way. Statistics tends to be an office art based on
machines and figures rather than a field art based on living things.

> **Henry A. Wallace and William L. Brown**, *Corn and Its Early Fathers*, 1956, page 123. [kw]



The great scientific weakness of America today is that she tends to emphasize
quantity at the expense of quality--statistics instead of genuine
insight--immediate utilitarian application instead of genuine thought about
fundamentals.

> **Henry A. Wallace and William L. Brown**, *Corn and Its Early Fathers*, 1956, page 124. [kw]



I'm still convinced the best goodness-of-fit test is your eyeball.

> **Bruce Stanley**, personal communication [kw]



Rejection of a true null hypothesis at the 0.05 level will occur only one in
20 times. The overwhelming majority of these false rejections will be based on
test statistics close to the borderline value. If the null hypothesis is
false, the inter-ocular traumatic test ["hit between the eyes"] will
often suffice to reject it; calculation will serve only to verify clear
intuition.

> **W. Edwards, Harold Lindman, Leonard J. Savage** (1962), *Bayesian Statistical Inference for Psychological Research*, University of Michigan, personal communication [kw]


The government are very keen on amassing statistics. They collect them, add
them, raise them to the n-th power, take the cube root and prepare wonderful
diagrams. But you must never forget that every one of these figures comes in
the first instance from the village watchman, who just puts down what he damn
pleases.

> **English judge** commenting on the subject of Indian statistics;
Quoted in Sir Josiah Stamp in Some Economic Matters in Modern Life,
London: King and Sons,1929, pp. 258-259.



It is difficult to understand why statisticians commonly limit their inquiries
to Averages, and do not revel in more comprehensive views.  Their souls seem
as dull to the charm of variety as that of the native of one of our flat
English counties, whose retrospect of Switzerland was that, if its mountains
could be thrown into its lakes, two nuisances would be got rid of at once. An
Average is but a solitary fact, whereas if a single other fact be added to it,
an entire Normal Scheme, which nearly corresponds to the observed one, starts
potentially into existence. Some people hate the very name of statistics, but
I find them full of beauty and interest. Whenever they are not brutalised, but
delicately handled by the higher methods, and are warily interpreted, their
power of dealing with complicated phenomena is extraordinary. They are the
only tools by which an opening can be cut through the formidable thicket of
difficulties that bars the path of those who pursue the Science of man.

> **Frances Galton**, *Natural Inheritance*. Found in *Francis Galton: Pioneer of Heredity and Biometry* by Michael Bulmer.


It has now been proved beyond doubt that smoking is one of the leading causes
of statistics.

> **Fletcher Krebel**


Bill Hunter told me that their editor wanted a title for their book with sex
appeal. Thus, *Statistics for Experimenters*, which is pretty subliminal
but it's there.

> **Robert Easterling**, *The American Statistician*, v 58, p 248. [kw]



You prepare yourself to win. You prepare yourself for the possibility that you
won't win. You don't really prepare yourself for the possibility that you flip
the coin in the air and it lands on its edge and you get neither outcome.

> **Al Gore**, On the 2004 presidential election. *Chance News* 10.01



It is easy to lie with statistics, but it is easier to lie without them.

> **Frederick Mosteller**



Too much of what all statisticians do ... is blatantly subjective for any of
us to kid ourselves or the users of our technology into believing that we have
operated 'impartially' in any true sense. ... We can do what seems to us most
appropriate, but we can not be objective and would do well to avoid language
that hints to the contrary.

> **Steve Vardeman**, 1987.  Comment.  *Journal of the American Statistical Association* 82 : 130-131.



It is really worthwhile to understand how to write covariance matrices in
terms of Kronecker products--it is just really beautiful.

> **DW** [kw]


Having given the number of instances respectively in which things are both
thus and so, in which they are thus but not so, in which they are so but not
thus, and in which they are neither thus not so, it is required to eliminate
the general quantitative relativity inhering in the mere thingness of the
things, and to determine the special quantitative relativity subsisting
between the thusness and the soness of the things.

> **M H Doolittle**, 1888. Association Ratios. *Bull Philosoph Soc Wash*, 7:122. Quoted in *Categorical Data Analysis*, by Alan Agresti.



Strangely, the motto chosen by the founders of the Statistical Society in 1834
was *Aliis exterendum*, which means "Let other thrash it out." William Cochran
confessed that "it is a little embaressing that statisticians started out by
proclaiming what they will not do.

> **Edmund A. Gehan and Noreen A. Lemak**, Statistics in Medical Research: Developments in Clinical Trials


If you show your friends your confidence interval for the standard error of
the estimated length of the confidence interval of your confidence about
yourself, I guess one nice thing to ask to freak them out is: "Can you
construct a confidence interval for the confidence level of my confidence?

> **Tony Baiching**, And Now Deep Thoughts (personal communication) [kw]



[Statistics] is not a subject because there is nothing revolutionary about it,
but to those leaders of industry who realize that in these days of keen
competition and cut prices, any weapon which increases the brain competition
and cut prices, any weapon which increases the brain value of judgment of
members of their staffs must be an advantage, the subject should have a
definite appeal.

> **E. S. Grumell**, (1935) Statistical Methods In Industry, With Special Reference to the Sampling of Coal and Other Materials, *Supplement to the Journal of the Royal Statistical Society*, Vol 2, No. 1.




The standard error of most statistics is proportional to 1 over the square
root of the sample size.  God did this, and there is nothing we can do to
change it.

> **Howard Wainer**, (1997) Improving Tabular Displays, With NAEP Tables as Examples and Inspirations, *Journal of Educational and Behavioral Statistics*, Vol 22, No. 1, pp. 1-30.


    People with a fear of numbers
      may find 'statistic'
    uncomfortably close
      to the word 'sadistic'.

> **Kevin Wright**, Stat Grook 1


    A picture is worth words a plenty
    A graph replaces statistics many

> **Kevin Wright**, Stat Grook 2


## Links, References

Gaither, Statistically Speaking: A Dictionary of Quotations.
http://www.angelfire.com/tx/StatBook/statistics.html

J. E. H. Shaw (2004). Some Quotable Quotes for Statistics.
http://www.ewartshaw.co.uk/
